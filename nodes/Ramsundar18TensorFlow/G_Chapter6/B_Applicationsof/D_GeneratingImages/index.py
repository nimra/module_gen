# Lawrence McAfee

# ~~~~~~~~ import ~~~~~~~~
from modules.node.HierNode import HierNode
from modules.node.LeafNode import LeafNode
from modules.node.Stage import Stage
from modules.node.block.CodeBlock import CodeBlock as cbk
from modules.node.block.ImageBlock import ImageBlock as ibk
from modules.node.block.MarkdownBlock import MarkdownBlock as mbk


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Generating Images with Variational Autoencoders
# The applications we’ve described thus far are all supervised learning problems. There
# are well-defined inputs and outputs, and the task remains (using a convolutional net‐
# work) to learn a sophisticated function mapping input to output. Are there unsuper‐
# vised learning problems that can be solved with convolutional networks? Recall that
# unsupervised learning requires “understanding” the structure of input datapoints. For
# image modeling, a good measure of understanding the structure of input images is
# being able to “sample” new images that come from the input distribution.
# What does “sampling” an image mean? To explain, let’s suppose we have a dataset of
# dog images. Sampling a new dog image requires the generation of a new image of a
# dog that is not in the training data! The idea is that we would like a picture of a dog
# that could have reasonably been included with the training data, but was not. How
# could we solve this task with convolutional networks?
# Perhaps we could train a model to take in word labels like “dog” and predict dog
# images. We might possibly be able to train a supervised model to solve this prediction
# problem, but the issue remains that our model could generate only one dog picture
# given the input label “dog.” Suppose now that we could attach a random tag to each
# dog—say “dog3422” or “dog9879.” Then all we’d need to do to get a new dog image
# would be to attach a new random tag, say “dog2221,” to get out a new picture of a dog.
# Variational autoencoders formalize these intuitions. Variational autoencoders consist
# of two convolutional networks: the encoder and decoder network. The encoder net‐
# work is used to transform an image into a flat “embedded” vector. The decoder net‐
# work is responsible for transforming the embedded vector into images. Noise is
# added to ensure that different images can be sampled by the decoder. Figure 6-13
# illustrates a variational autoencoder.
# 
# 
# 
# 
# Figure 6-13. A diagrammatic illustration of a variational autoencoder. A variational
# autoencoder consists of two convolutional networks, the encoder and decoder.
# 
# There are more details involved in an actual implementation, but variational autoen‐
# coders are capable of sampling images. However, naive variational encoders seem to
# generate blurry image samples, as Figure 6-14 demonstrates. This blurriness may be
# 
# 
#                                                     Applications of Convolutional Networks   |   131
# 
# because the L2 loss doesn’t penalize image blurriness sharply (recall our discussion
# about L2 not penalizing small deviations). To generate crisp image samples, we will
# need other architectures.
# 
# 
# 
# 
# Figure 6-14. Images sampled from a variational autoencoder trained on a dataset of
# faces. Note that sampled images are quite blurry.
# 
# Adversarial models
# The L2 loss sharply penalizes large local deviations, but doesn’t severely penalize
# many small local deviations, causing blurriness. How could we design an alternate
# loss function that penalizes blurriness in images more sharply? It turns out that it’s
# quite challenging to write down a loss function that does the trick. While our eyes can
# quickly spot blurriness, our analytical tools aren’t quite so fast to capture the problem.
# What if we could somehow “learn” a loss function? This idea sounds a little nonsensi‐
# cal at first; where would we get training data? But it turns out that there’s a clever idea
# that makes it feasible.
# Suppose we could train a separate network that learns the loss. Let’s call this network
# the discriminator. Let’s call the network that makes the images the generator. The
# generator can be set to duel against the discriminator until the generator is capable of
# producing images that are photorealistic. This form of architecture is commonly
# called a generative adversarial network, or GAN.
# 
# 
# 
# 
# 132   |   Chapter 6: Convolutional Neural Networks
# 
# Faces generated by a GAN (Figure 6-15) are considerably crisper than those gener‐
# ated by the naive variational autoencoder (Figure 6-14)! There are a number of other
# promising results that have been achieved by GANs. The CycleGAN, for example,
# appears capable of learning complex image transformations such as transmuting
# horses into zebras and vice versa. Figure 6-16 shows some CycleGAN image transfor‐
# mations.
# 
# 
# 
# 
# Figure 6-15. Images sampled from a generative adversarial network (GAN) trained on a
# dataset of faces. Note that sampled images are less blurry than those from the variational
# autoencoder.
# 
# 
# 
# 
#                                                      Applications of Convolutional Networks   |   133
# 
# Figure 6-16. The CycleGAN is capable of performing complex image transformations,
# such as transforming images of horses into those of zebras (and vice versa).
# 
# Unfortunately, generative adversarial networks are still challenging to train in prac‐
# tice. Making generators and discriminators learn reasonable functions requires a
# deep bag of tricks. As a result, while there have been many exciting GAN demonstra‐
# tions, GANs have not yet matured into a state where they can be widely deployed in
# industrial applications.
# 
# Training a Convolutional Network in TensorFlow
# In this section we consider a code sample for training a simple convolutional neural
# network. In particular, our code sample will demonstrate how to train a LeNet-5 con‐
# volutional architecture on the MNIST dataset using TensorFlow. As always, we rec‐
# ommend that you follow along by running the full code sample from the GitHub
# repo associated with the book.
# 
# The MNIST Dataset
# The MNIST dataset consists of images of handwritten digits. The machine learning
# challenge associated with MNIST consists of creating a model trained on the training
# set of digits that generalizes to the validation set. Figure 6-17 shows some images
# drawn from the MNIST dataset.
# 
# 
# 
# 
# 134   |   Chapter 6: Convolutional Neural Networks
# 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class Content(LeafNode):
    def __init__(self):
        super().__init__(
            "Generating Images with Variational Autoencoders",
            # Stage.CROP_TEXT,
            # Stage.CODE_BLOCKS,
            # Stage.MARKDOWN_BLOCKS,
            # Stage.FIGURES,
            # Stage.EXERCISES,
            # Stage.CUSTOMIZED,
        )
        self.add(mbk("# Generating Images with Variational Autoencoders"))

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class GeneratingImages(HierNode):
    def __init__(self):
        super().__init__("Generating Images with Variational Autoencoders")
        self.add(Content())

# eof
