AlphaGo
Go is an ancient board game, widely influential in Asia. Computer Go has been a
major challenge for computer science since the late 1960s. Techniques that enabled
the computer chess system Deep Blue to beat chess grandmaster Garry Kasparov in
1997 don’t scale to Go. Part of the issue is that Go has a much bigger board than
chess; Go boards are of size 19 × 19 as opposed to 8 × 8 for chess. Since far more
moves are possible per step, the game tree of possible Go moves expands much more
quickly, rendering brute force search with contemporary computer hardware insuffi‐
cient for adequate Go gameplay. Figure 1-11 illustrates a Go board.




Figure 1-11. An illustration of a Go board. Players alternately place white and black
pieces on a 19 × 19 grid.

Master level computer Go was finally achieved by AlphaGo from Google DeepMind.
AlphaGo proved capable of defeating one of the world’s strongest Go champions, Lee
Sedol, in a five-game match. Some of the key ideas from AlphaGo include the use of a
deep value network and deep policy network. The value network provides an esti‐
mate of the value of a board position. Unlike chess, it’s very difficult to guess whether
white or black is winning in Go from the board state. The value network solves this
problem by learning to make this prediction from game outcomes. The policy net‐
work, on the other hand, helps estimate the best move to take given a current board
state. The combination of these two techniques with Monte Carlo Tree search (a clas‐
sical search method) helped overcome the large branching factor in Go games. The
basic AlphaGo architecture is illustrated in Figure 1-12.




12   |   Chapter 1: Introduction to Deep Learning
Figure 1-12. A) Depiction of AlphaGo’s architecture. Initially a policy network to select
moves is trained on a dataset of expert games. This policy is then refined by self-play.
“RL” indicates reinforcement learning and “SL” indicates supervised learning. B) Both the
policy and value networks operate on representations of the game board.

Generative Adversarial Networks
Generative adversarial networks (GANs) are a new type of deep network that uses
two competing neural networks, the generator and the adversary (also called the dis‐
criminator), which duel against each other. The generator tries to draw samples from
a training distribution (for example, tries to generate realistic images of birds). The
discriminator works on differentiating samples drawn from the generator from true
data samples. (Is a particular bird a real image or generator-created?) This “adversa‐
rial” training for GANs seems capable of generating image samples of considerably
higher fidelity than other techniques and may be useful for training effective discrim‐
inators with limited data. A GAN architecture is illustrated in Figure 1-13.




                                                             Deep Learning Architectures   |   13
