      Every 700th feature:
      ['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered',
       'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds',
       'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate',
       'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking',
       'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea',
       'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']
There are clearly many fewer numbers, and some of the more obscure words or mis‐
spellings seem to have vanished. Let’s see how well our model performs by doing a
grid search again:
In[19]:
      grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
      grid.fit(X_train, y_train)
      print("Best cross-validation score: {:.2f}".format(grid.best_score_))

Out[19]:
      Best cross-validation score: 0.89
The best validation accuracy of the grid search is still 89%, unchanged from before.
We didn’t improve our model, but having fewer features to deal with speeds up pro‐
cessing and throwing away useless features might make the model more interpretable.

                    If the transform method of CountVectorizer is called on a docu‐
                    ment that contains words that were not contained in the training
                    data, these words will be ignored as they are not part of the dictio‐
                    nary. This is not really an issue for classification, as it’s not possible
                    to learn anything about words that are not in the training data. For
                    some applications, like spam detection, it might be helpful to add a
                    feature that encodes how many so-called “out of vocabulary” words
                    there are in a particular document, though. For this to work, you
                    need to set min_df; otherwise, this feature will never be active dur‐
                    ing training.


Stopwords
Another way that we can get rid of uninformative words is by discarding words that
are too frequent to be informative. There are two main approaches: using a language-
specific list of stopwords, or discarding words that appear too frequently. scikit-
learn has a built-in list of English stopwords in the feature_extraction.text
module:




334   |   Chapter 7: Working with Text Data
In[20]:
   from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
   print("Number of stop words: {}".format(len(ENGLISH_STOP_WORDS)))
   print("Every 10th stopword:\n{}".format(list(ENGLISH_STOP_WORDS)[::10]))

Out[20]:
   Number of stop words: 318
   Every 10th stopword:
   ['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',
    'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',
    'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',
    'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']
Clearly, removing the stopwords in the list can only decrease the number of features
by the length of the list—here, 318—but it might lead to an improvement in perfor‐
mance. Let’s give it a try:
In[21]:
   # Specifying stop_words="english" uses the built-in list.
   # We could also augment it and pass our own.
   vect = CountVectorizer(min_df=5, stop_words="english").fit(text_train)
   X_train = vect.transform(text_train)
   print("X_train with stop words:\n{}".format(repr(X_train)))

Out[21]:
   X_train with stop words:
   <25000x26966 sparse matrix of type '<class 'numpy.int64'>'
       with 2149958 stored elements in Compressed Sparse Row format>
There are now 305 (27,271–26,966) fewer features in the dataset, which means that
most, but not all, of the stopwords appeared. Let’s run the grid search again:
In[22]:
   grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)
   grid.fit(X_train, y_train)
   print("Best cross-validation score: {:.2f}".format(grid.best_score_))

Out[22]:
   Best cross-validation score: 0.88
The grid search performance decreased slightly using the stopwords—not enough to
worry about, but given that excluding 305 features out of over 27,000 is unlikely to
change performance or interpretability a lot, it doesn’t seem worth using this list.
Fixed lists are mostly helpful for small datasets, which might not contain enough
information for the model to determine which words are stopwords from the data
itself. As an exercise, you can try out the other approach, discarding frequently




                                                                       Stopwords   |   335
appearing words, by setting the max_df option of CountVectorizer and see how it
influences the number of features and the performance.

Rescaling the Data with tf–idf
Instead of dropping features that are deemed unimportant, another approach is to
rescale features by how informative we expect them to be. One of the most common
ways to do this is using the term frequency–inverse document frequency (tf–idf)
method. The intuition of this method is to give high weight to any term that appears
often in a particular document, but not in many documents in the corpus. If a word
appears often in a particular document, but not in very many documents, it is likely
to be very descriptive of the content of that document. scikit-learn implements the
tf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix
output produced by CountVectorizer and transforms it, and TfidfVectorizer,
which takes in the text data and does both the bag-of-words feature extraction and
the tf–idf transformation. There are several variants of the tf–idf rescaling scheme,
which you can read about on Wikipedia. The tf–idf score for word w in document d
as implemented in both the TfidfTransformer and TfidfVectorizer classes is given
by:7

                                N+1
      tfidf w, d = tf log              +1
                                Nw + 1

where N is the number of documents in the training set, Nw is the number of docu‐
ments in the training set that the word w appears in, and tf (the term frequency) is the
number of times that the word w appears in the query document d (the document
you want to transform or encode). Both classes also apply L2 normalization after
computing the tf–idf representation; in other words, they rescale the representation
of each document to have Euclidean norm 1. Rescaling in this way means that the
length of a document (the number of words) does not change the vectorized repre‐
sentation.
Because tf–idf actually makes use of the statistical properties of the training data, we
will use a pipeline, as described in Chapter 6, to ensure the results of our grid search
are valid. This leads to the following code:




7 We provide this formula here mostly for completeness; you don’t need to remember it to use the tf–idf
  encoding.



336    |   Chapter 7: Working with Text Data
