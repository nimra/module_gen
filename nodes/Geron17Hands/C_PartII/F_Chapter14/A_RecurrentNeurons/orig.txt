                       Download from finelybook www.finelybook.com
Recurrent Neurons
Up to now we have mostly looked at feedforward neural networks, where the activa‐
tions flow only in one direction, from the input layer to the output layer (except for a
few networks in Appendix E). A recurrent neural network looks very much like a
feedforward neural network, except it also has connections pointing backward. Let’s
look at the simplest possible RNN, composed of just one neuron receiving inputs,
producing an output, and sending that output back to itself, as shown in Figure 14-1
(left). At each time step t (also called a frame), this recurrent neuron receives the inputs
x(t) as well as its own output from the previous time step, y(t–1). We can represent this
tiny network against the time axis, as shown in Figure 14-1 (right). This is called
unrolling the network through time.




Figure 14-1. A recurrent neuron (left), unrolled through time (right)

You can easily create a layer of recurrent neurons. At each time step t, every neuron
receives both the input vector x(t) and the output vector from the previous time step
y(t–1), as shown in Figure 14-2. Note that both the inputs and outputs are vectors now
(when there was just a single neuron, the output was a scalar).




Figure 14-2. A layer of recurrent neurons (left), unrolled through time (right)

Each recurrent neuron has two sets of weights: one for the inputs x(t) and the other for
the outputs of the previous time step, y(t–1). Let’s call these weight vectors wx and wy.


380   |   Chapter 14: Recurrent Neural Networks
                   Download from finelybook www.finelybook.com
The output of a single recurrent neuron can be computed pretty much as you might
expect, as shown in Equation 14-1 (b is the bias term and ϕ(·) is the activation func‐
tion, e.g., ReLU1).

    Equation 14-1. Output of a single recurrent neuron for a single instance
                   T                    T
    �t =ϕ�t            · �x + � t − 1       · �y + b

Just like for feedforward neural networks, we can compute a whole layer’s output in
one shot for a whole mini-batch using a vectorized form of the previous equation (see
Equation 14-2).

    Equation 14-2. Outputs of a layer of recurrent neurons for all instances in a mini-
    batch

    � t = ϕ � t · �x + � t − 1 · � y + �
                                                              �x
          =ϕ �t           � t − 1 · � + � with � =
                                                              �y


  • Y(t) is an m × nneurons matrix containing the layer’s outputs at time step t for each
    instance in the mini-batch (m is the number of instances in the mini-batch and
    nneurons is the number of neurons).
  • X(t) is an m × ninputs matrix containing the inputs for all instances (ninputs is the
    number of input features).
  • Wx is an ninputs × nneurons matrix containing the connection weights for the inputs
    of the current time step.
  • Wy is an nneurons × nneurons matrix containing the connection weights for the out‐
    puts of the previous time step.
  • The weight matrices Wx and Wy are often concatenated into a single weight
    matrix W of shape (ninputs + nneurons) × nneurons (see the second line of Equation
    14-2).
  • b is a vector of size nneurons containing each neuron’s bias term.




1 Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather
  than the ReLU activation function. For example, take a look at by Vu Pham et al.’s paper “Dropout Improves
  Recurrent Neural Networks for Handwriting Recognition”. However, ReLU-based RNNs are also possible, as
  shown in Quoc V. Le et al.’s paper “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”.



                                                                                     Recurrent Neurons   |    381
                   Download from finelybook www.finelybook.com
Notice that Y(t) is a function of X(t) and Y(t–1), which is a function of X(t–1) and Y(t–2),
which is a function of X(t–2) and Y(t–3), and so on. This makes Y(t) a function of all the
inputs since time t = 0 (that is, X(0), X(1), …, X(t)). At the first time step, t = 0, there are
no previous outputs, so they are typically assumed to be all zeros.

Memory Cells
Since the output of a recurrent neuron at time step t is a function of all the inputs
from previous time steps, you could say it has a form of memory. A part of a neural
network that preserves some state across time steps is called a memory cell (or simply
a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell,
but later in this chapter we will look at some more complex and powerful types of
cells.
In general a cell’s state at time step t, denoted h(t) (the “h” stands for “hidden”), is a
function of some inputs at that time step and its state at the previous time step: h(t) =
f(h(t–1), x(t)). Its output at time step t, denoted y(t), is also a function of the previous
state and the current inputs. In the case of the basic cells we have discussed so far, the
output is simply equal to the state, but in more complex cells this is not always the
case, as shown in Figure 14-3.




Figure 14-3. A cell’s hidden state and its output may be different

Input and Output Sequences
An RNN can simultaneously take a sequence of inputs and produce a sequence of
outputs (see Figure 14-4, top-left network). For example, this type of network is use‐
ful for predicting time series such as stock prices: you feed it the prices over the last N
days, and it must output the prices shifted by one day into the future (i.e., from N – 1
days ago to tomorrow).
Alternatively, you could feed the network a sequence of inputs, and ignore all outputs
except for the last one (see the top-right network). In other words, this is a sequence-
to-vector network. For example, you could feed the network a sequence of words cor‐


382   |   Chapter 14: Recurrent Neural Networks
