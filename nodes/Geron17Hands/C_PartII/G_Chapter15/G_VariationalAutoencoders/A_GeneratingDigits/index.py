# Lawrence McAfee

# ~~~~~~~~ import ~~~~~~~~
from modules.node.HierNode import HierNode
from modules.node.LeafNode import LeafNode
from modules.node.Stage import Stage
from modules.node.block.CodeBlock import CodeBlock
from modules.node.block.MarkdownBlock import MarkdownBlock


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                  Download from finelybook www.finelybook.com
#         hidden1 = fully_connected(X, n_hidden1)
#         hidden2 = fully_connected(hidden1, n_hidden2)
#         hidden3_mean = fully_connected(hidden2, n_hidden3, activation_fn=None)
#         hidden3_gamma = fully_connected(hidden2, n_hidden3, activation_fn=None)
#         hidden3_sigma = tf.exp(0.5 * hidden3_gamma)
#         noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)
#         hidden3 = hidden3_mean + hidden3_sigma * noise
#         hidden4 = fully_connected(hidden3, n_hidden4)
#         hidden5 = fully_connected(hidden4, n_hidden5)
#         logits = fully_connected(hidden5, n_outputs, activation_fn=None)
#         outputs = tf.sigmoid(logits)
# 
#     reconstruction_loss = tf.reduce_sum(
#         tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits))
#     latent_loss = 0.5 * tf.reduce_sum(
#         tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)
#     cost = reconstruction_loss + latent_loss
# 
#     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
#     training_op = optimizer.minimize(cost)
# 
#     init = tf.global_variables_initializer()
# 
# 
# Generating Digits
# Now let’s use this variational autoencoder to generate images that look like handwrit‐
# ten digits. All we need to do is train the model, then sample random codings from a
# Gaussian distribution and decode them.
#     import numpy as np
# 
#     n_digits = 60
#     n_epochs = 50
#     batch_size = 150
# 
#     with tf.Session() as sess:
#         init.run()
#         for epoch in range(n_epochs):
#             n_batches = mnist.train.num_examples // batch_size
#             for iteration in range(n_batches):
#                 X_batch, y_batch = mnist.train.next_batch(batch_size)
#                 sess.run(training_op, feed_dict={X: X_batch})
# 
#         codings_rnd = np.random.normal(size=[n_digits, n_hidden3])
#         outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})
# That’s it. Now we can see what the “handwritten” digits produced by the autoencoder
# look like (see Figure 15-12):
#     for iteration in range(n_digits):
#         plt.subplot(n_digits, 10, iteration + 1)
#         plot_image(outputs_val[iteration])
# 
# 
# 
#                                                             Variational Autoencoders   |   431
# 
#                        Download from finelybook www.finelybook.com
# 
# 
# 
# 
# Figure 15-12. Images of handwritten digits generated by the variational autoencoder
# 
# A majority of these digits look pretty convincing, while a few are rather “creative.” But
# don’t be too harsh on the autoencoder—it only started learning less than an hour ago.
# Give it a bit more training time, and those digits will look better and better.
# 
# Other Autoencoders
# The amazing successes of supervised learning in image recognition, speech recogni‐
# tion, text translation, and more have somewhat overshadowed unsupervised learning,
# but it is actually booming. New architectures for autoencoders and other unsuper‐
# vised learning algorithms are invented regularly, so much so that we cannot cover
# them all in this book. Here is a brief (by no means exhaustive) overview of a few more
# types of autoencoders that you may want to check out:
# Contractive autoencoder (CAE)8
#    The autoencoder is constrained during training so that the derivatives of the cod‐
#    ings with regards to the inputs are small. In other words, two similar inputs must
#    have similar codings.
# 
# 
# 
# 
# 8 “Contractive Auto-Encoders: Explicit Invariance During Feature Extraction,” S. Rifai et al. (2011).
# 
# 
# 
# 432   |   Chapter 15: Autoencoders
# 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class Content(LeafNode):
    def __init__(self):
        super().__init__(
            "Generating Digits",
            # Stage.CROP_TEXT,
            # Stage.CODE_BLOCKS,
            # Stage.MARKDOWN_BLOCKS,
            # Stage.FIGURES,
            # Stage.EXERCISES,
            # Stage.CUSTOMIZED,
        )
        self.add(MarkdownBlock("# Generating Digits"))

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class GeneratingDigits(HierNode):
    def __init__(self):
        super().__init__("Generating Digits")
        self.add(Content())

# eof
