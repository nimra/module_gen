                  Download from finelybook www.finelybook.com
                The direction of the principal components is not stable: if you per‐
                turb the training set slightly and run PCA again, some of the new
                PCs may point in the opposite direction of the original PCs. How‐
                ever, they will generally still lie on the same axes. In some cases, a
                pair of PCs may even rotate or swap, but the plane they define will
                generally remain the same.

So how can you find the principal components of a training set? Luckily, there is a
standard matrix factorization technique called Singular Value Decomposition (SVD)
that can decompose the training set matrix X into the dot product of three matrices U
· Σ · VT, where VT contains all the principal components that we are looking for, as
shown in Equation 8-1.

   Equation 8-1. Principal components matrix
        ∣ ∣     ∣
     T
   � = �1 �2 ⋯ ��
          ∣ ∣        ∣

The following Python code uses NumPy’s svd() function to obtain all the principal
components of the training set, then extracts the first two PCs:
    X_centered = X - X.mean(axis=0)
    U, s, V = np.linalg.svd(X_centered)
    c1 = V.T[:, 0]
    c2 = V.T[:, 1]

                PCA assumes that the dataset is centered around the origin. As we
                will see, Scikit-Learn’s PCA classes take care of centering the data
                for you. However, if you implement PCA yourself (as in the pre‐
                ceding example), or if you use other libraries, don’t forget to center
                the data first.


Projecting Down to d Dimensions
Once you have identified all the principal components, you can reduce the dimen‐
sionality of the dataset down to d dimensions by projecting it onto the hyperplane
defined by the first d principal components. Selecting this hyperplane ensures that the
projection will preserve as much variance as possible. For example, in Figure 8-2 the
3D dataset is projected down to the 2D plane defined by the first two principal com‐
ponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐
tion looks very much like the original 3D dataset.
To project the training set onto the hyperplane, you can simply compute the dot
product of the training set matrix X by the matrix Wd, defined as the matrix contain‐

                                                                                    PCA   |   213
                   Download from finelybook www.finelybook.com
ing the first d principal components (i.e., the matrix composed of the first d columns
of VT), as shown in Equation 8-2.

      Equation 8-2. Projecting the training set down to d dimensions
      �d‐proj = � · �d

The following Python code projects the training set onto the plane defined by the first
two principal components:
      W2 = V.T[:, :2]
      X2D = X_centered.dot(W2)
There you have it! You now know how to reduce the dimensionality of any dataset
down to any number of dimensions, while preserving as much variance as possible.

Using Scikit-Learn
Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did
before. The following code applies PCA to reduce the dimensionality of the dataset
down to two dimensions (note that it automatically takes care of centering the data):
      from sklearn.decomposition import PCA

      pca = PCA(n_components = 2)
      X2D = pca.fit_transform(X)

After fitting the PCA transformer to the dataset, you can access the principal compo‐
nents using the components_ variable (note that it contains the PCs as horizontal vec‐
tors, so, for example, the first principal component is equal to pca.components_.T[:,
0]).

Explained Variance Ratio
Another very useful piece of information is the explained variance ratio of each prin‐
cipal component, available via the explained_variance_ratio_ variable. It indicates
the proportion of the dataset’s variance that lies along the axis of each principal com‐
ponent. For example, let’s look at the explained variance ratios of the first two compo‐
nents of the 3D dataset represented in Figure 8-2:
      >>> print(pca.explained_variance_ratio_)
      array([ 0.84248607, 0.14631839])
This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%
lies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐
able to assume that it probably carries little information.




214    |   Chapter 8: Dimensionality Reduction
