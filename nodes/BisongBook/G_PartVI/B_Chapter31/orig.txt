CHAPTER 31



The Multilayer
Perceptron (MLP)
The multilayer perceptron (MLP) is the fundamental example of a deep neural network.
The architecture of a MLP consists of multiple hidden layers to capture more complex
relationships that exist in the training dataset. Another name for the MLP is the deep
feedforward neural network (DFN). An illustration of an MLP is shown in Figure 31-1.




Figure 31-1. Deep feedforward neural network


The Concept of Hierarchies
The more the number of hidden layers in a neural network, the deeper the network
becomes. Deep networks are able to learn more sophisticated representations of the
inputs. The concept of hierarchical representation is when each layer learns a set of
features that describe the input and hierarchically pass that information across the
hidden layers. Initially, the hidden layers closer to the input layer learn a simple set

                                                                                           401
Â© Ekaba Bisong 2019
E. Bisong, Building Machine Learning and Deep Learning Models on Google Cloud Platform,
https://doi.org/10.1007/978-1-4842-4470-8_31
