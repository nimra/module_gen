# Lawrence McAfee

# ~~~~~~~~ import ~~~~~~~~
from modules.node.HierNode import HierNode
from modules.node.LeafNode import LeafNode
from modules.node.Stage import Stage
from modules.node.block.CodeBlock import CodeBlock
from modules.node.block.MarkdownBlock import MarkdownBlock

from .A_AddingPolynomial.index import AddingPolynomial as A_AddingPolynomial
from .B_Kernels.index import Kernels as B_Kernels

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Chapter 22   Support Vector Machines
# 
# 
# 
# 
# Figure 22-9. Given four classes in a dataset, we construct four classifiers, with
# each class fitted against the rest
# 
#     The classifiers are evaluated by comparing a test example to each fitted classifier. The
# classifier for which the margin of the hyperplane is the largest is chosen as the predicted
# classification target because the classifier margin size is indicative of high confidence of
# class membership.
# 
# 
# 
# T he Kernel Trick: Fitting Non-linear Decision
#  Boundaries
# Non-linear datasets occur more often than not in real world scenarios.
#     Technically speaking, the name support vector machine is when a support vector
# classifier is used with a non-linear kernel to learn non-linear decision boundaries.
# 
# 
# 
# 262
# 
#                                                        Chapter 22   Support Vector Machines
# 
#     SVM uses an essential technique for extending the feature space of a dataset to
# construct a non-linear classifier. This technique is called kernel and is popularly known
# as the kernel trick. Figure 22-10 illustrates the kernel trick as an extra dimension is added
# to the feature space.
# 
# 
# 
# 
# Figure 22-10. Left: Linear discriminant to non-linear data. Right: By using the
# kernel trick, we can linearly separate a non-linear dataset by adding an extra
# dimension to the feature space.
# 
# 
# Adding Polynomial Features
# The feature space of the dataset can be extended by adding higher-order polynomial
# terms or interaction terms. For example, instead of training the classifier with linear
# features, we can add polynomial features or add interaction terms to our model.
#     Depending on the dimensions of the dataset, the combinations for extending
# the feature space can quickly become unmanageable, and this can easily lead to a
# model that overfits the test set and also become expensive to compute with a larger
# feature space.
# 
# 
# 
# 
#                                                                                          263
# 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class Content(LeafNode):
    def __init__(self):
        super().__init__(
            "The Kernel Trick: Fitting Non-linear Decision Boundaries",
            # Stage.CLEAN_TEXT,
            # Stage.CODE_BLOCKS,
            # Stage.MARKDOWN_BLOCKS,
            # Stage.FIGURES,
            # Stage.EXERCISES,
            # Stage.CUSTOMIZED,
        )
        self.add(MarkdownBlock("# The Kernel Trick: Fitting Non-linear Decision Boundaries"))

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class TheKernel(HierNode):
    def __init__(self):
        super().__init__("The Kernel Trick: Fitting Non-linear Decision Boundaries")
        self.add(Content())
        self.add(A_AddingPolynomial())
        self.add(B_Kernels())

# eof
