# Lawrence McAfee

# ~~~~~~~~ import ~~~~~~~~
from modules.node.HierNode import HierNode
from modules.node.LeafNode import LeafNode
from modules.node.Stage import Stage
from modules.node.block.CodeBlock import CodeBlock
from modules.node.block.MarkdownBlock import MarkdownBlock

from .A_HowAre.index import HowAre as A_HowAre
from .B_DimensionalityReduction.index import DimensionalityReduction as B_DimensionalityReduction
from .C_KeyConsiderations.index import KeyConsiderations as C_KeyConsiderations
from .D_PCAwith.index import PCAwith as D_PCAwith

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# CHAPTER 26
# 
# 
# 
# Principal Component
# Analysis (PCA)
# Principal component analysis (PCA) is an essential algorithm in machine learning. It
# is a mathematical method for evaluating the principal components of a dataset. The
# principal components are a set of vectors in high-dimensional space that capture the
# variance (i.e., spread) or variability of the feature space.
#      The goal of computing principal components is to find a low-dimensional feature
# sub-space that captures as much information as possible from the original higher-­
# dimensional features of the dataset.
#      PCA is particularly useful for simplifying data visualization of high-dimensional
# features by reducing the dimensions of the dataset to a lower sub-space. For example,
# since we can easily visualize relationships on a 2-D plane using scatter diagrams, it will
# be useful to condense an n-dimensional space into two dimensions that retain as much
# information as possible in the n-dimensional dataset. This technique is popularly called
# dimensionality reduction.
# 
# 
# How Are Principal Components Computed
# The mathematical details for computing principal components are somewhat involved.
# This section will instead provide a conceptual but solid overview of this process.
#     The first step is to find the covariance matrix of the dataset. The covariance matrix
# captures the linear relationship between variables or features in the dataset. In a
# covariance matrix, an increasingly positive number represents a growing relationship,
# while the converse is represented by an increasingly negative number. Numbers around
# zero indicate a non-linear relationship between the variables. The covariance matrix is
# a square matrix (that means it has the same rows and columns). Hence, given a dataset
# with m rows and p columns, the covariance matrix will be a m × p matrix.
#                                                                                          319
# © Ekaba Bisong 2019
# E. Bisong, Building Machine Learning and Deep Learning Models on Google Cloud Platform,
# https://doi.org/10.1007/978-1-4842-4470-8_26
# 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class Content(LeafNode):
    def __init__(self):
        super().__init__(
            "Chapter 26: Principal Component Analysis (PCA)",
            # Stage.CLEAN_TEXT,
            # Stage.CODE_BLOCKS,
            # Stage.MARKDOWN_BLOCKS,
            # Stage.FIGURES,
            # Stage.EXERCISES,
            # Stage.CUSTOMIZED,
        )
        self.add(MarkdownBlock("# Chapter 26: Principal Component Analysis (PCA)"))

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class Chapter26(HierNode):
    def __init__(self):
        super().__init__("Chapter 26: Principal Component Analysis (PCA)")
        self.add(Content())
        self.add(A_HowAre())
        self.add(B_DimensionalityReduction())
        self.add(C_KeyConsiderations())
        self.add(D_PCAwith())

# eof
